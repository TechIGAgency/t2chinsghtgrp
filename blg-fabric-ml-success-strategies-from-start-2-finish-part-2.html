<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <title>Tech-Insight-Group – Advancing Data, AI & Visualization Through Consulting and Training Services</title>
        
        <!-- startFavicon / Browser tab icon -->
        <!-- Using your image at img/Browser-tab-icon.jpeg. For best results consider adding a .ico or PNG sized versions (favicon.ico, favicon-32x32.png). -->
        <link rel="icon" href="img/Browser-tab-icon-live-gd.png" type="image/png">
        <link rel="shortcut icon" href="img/Browser-tab-icon-live-gd.png" type="image/png">
        <link rel="apple-touch-icon" href="img/Browser-tab-icon-live-gd.png">       
        <!-- end Favicon / Browser tab icon -->
                
        <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="" name="keywords">
        <meta content="" name="description">

        <!-- Google Web Fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Saira:wght@500;600;700&display=swap" rel="stylesheet"> 

        <!-- Icon Font Stylesheet -->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.4.1/font/bootstrap-icons.css" rel="stylesheet">

        <!-- Libraries Stylesheet -->
        <link href="lib/animate/animate.min.css" rel="stylesheet">
        <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

        <!-- Customized Bootstrap Stylesheet -->
        <link href="css/bootstrap.min.css" rel="stylesheet">

        <!-- Template Stylesheet -->
        <link href="css/style.css" rel="stylesheet">
    </head>

    <body>
        <!-- Spinner Start -->
        <div id="spinner" class="show position-fixed translate-middle w-100 vh-100 top-50 start-50 d-flex align-items-center justify-content-center">
            <div class="spinner-grow text-primary" role="status"></div>
        </div>
        <!-- Spinner End -->

        <!-- Topbar Start -->
        <div id="topbar"></div>
        <script src="dislmr/top-bar.js"></script>
        <script>
            loadTopbar('topbar'); // or loadTopbar('topbar') if you want to be explicit
        </script>
        <!-- Topbar End -->

        <!-- Navbar Start -->
        <div id="navbar"></div>

         <script src="dislmr/navbar.js"></script>
         <script>
           loadNavbar('navbar'); // or loadNavbar('navbar') if you want to be explicit
         </script>
        <!-- Navbar End -->

        
        <!-- Page Header Start -->
        <div class="container-fluid page-header py-5">
            <div class="container text-center py-5">
                <h1 class="display-2 text-white mb-4 animated slideInDown">Fabric ML Lifecycle: Success Strategies from Start to Finish - Part-2</h1>
                <!--<nav aria-label="breadcrumb animated slideInDown">
                    <ol class="breadcrumb justify-content-center mb-0">
                        <li class="breadcrumb-item"><a href="#">Home</a></li>
                        <li class="breadcrumb-item"><a href="#">Pages</a></li>
                        <li class="breadcrumb-item" aria-current="page">Services</li>
                    </ol>
                </nav>-->
            </div>
        </div>
        <!-- Page Header End -->


        <!-- Fact Start -->
         <!-- 
        <div class="container-fluid bg-secondary py-5">
            <div class="container">
                <div class="row">
                    <div class="col-lg-3 wow fadeIn" data-wow-delay=".1s">
                        <div class="d-flex counter">
                            <h1 class="me-3 text-primary counter-value">99</h1>
                            <h5 class="text-white mt-1">Success in getting happy customer</h5>
                        </div>
                    </div>
                    <div class="col-lg-3 wow fadeIn" data-wow-delay=".3s">
                        <div class="d-flex counter">
                            <h1 class="me-3 text-primary counter-value">25</h1>
                            <h5 class="text-white mt-1">Thousands of successful business</h5>
                        </div>
                    </div>
                    <div class="col-lg-3 wow fadeIn" data-wow-delay=".5s">
                        <div class="d-flex counter">
                            <h1 class="me-3 text-primary counter-value">120</h1>
                            <h5 class="text-white mt-1">Total clients who love HighTech</h5>
                        </div>
                    </div>
                    <div class="col-lg-3 wow fadeIn" data-wow-delay=".7s">
                        <div class="d-flex counter">
                            <h1 class="me-3 text-primary counter-value">5</h1>
                            <h5 class="text-white mt-1">Stars reviews given by satisfied clients</h5>
                        </div>
                    </div>
                </div>
            </div>
        </div> -->
        <!-- Fact End -->

    <style> .zoom-hover { transition: transform 0.3s ease; } .zoom-hover:hover { transform: scale(1.10); } </style>

        <!-- Services Start -->

<div class="container-fluid py-5 my-5 d-flex justify-content-center">
    <div class="container pt-5" style="max-width: 1100px;">
        <div class="row justify-content-center">
            <div class="col-12 wow fadeIn" data-wow-delay=".5s">
                <!-- Your full content goes here -->

                <h2 class="text-primary mt-4">Fabric ML Lifecycle: Success Strategies from Start to Finish - Part-2 <br> <span class="text-secondary">Exploratory Data Analysis (EDA) & Pre-Processing</Pre-Processing></span></h2>
                    <img src="img/fb_ml_eda_0.png" class="img-fluid w-100 rounded" alt="">
                    <h3 class="text-primary mt-4"><b>Table of Contents:</b></h3>
                    <ul class="mb-4">
                        
                        <li style="color: #000000;"><b>Introduction to EDA :</b> Why EDA matters and how it shapes modeling decisions.</li>
                        <li style="color: #000000;"><b>Dataset Understanding:</b> Quick scan of structure, columns, and data types.</li>
                        <li style="color: #000000;"><b>Schema & Completeness Checks:</b> Identify missing, inconsistent, or unexpected values.</li>
                        <li style="color: #000000;"><b>Missing Value Imputation:</b> Apply simple, reliable strategies to fill gaps.</li>
                        <li style="color: #000000;"><b>Distribution & Outlier Analysis:</b> Spot skew, variability, and unusual patterns.</li>
                        <li style="color: #000000;"><b>Frequency Tables:</b> Understand categorical balance and dominant groups.</li>
                        <li style="color: #000000;"><b>Correlation Analysis:</b> Reveal relationships that influence the target.</li>
                        <li style="color: #000000;"><b>Feature Engineering Prep:</b> Prepare data for encoding and scaling.</li>
                        <li style="color: #000000;"><b>Save Cleaned Dataset:</b> Produce a ready‑to‑model dataset for the next phase.</li>
                        <li style="color: #000000;">Partnering with <a href="https://www.techinsightgroup.com/">Tech-Insight-Group</a>: Your Data Implementation Ally</li>                      <li style="color: #000000;">Call to action</li>
                    </ul>


                <p style="color: #000000; text-align: justify;"><a href="https://www.linkedin.com/company/tech-insight-group/?viewAsMember=true">Stay in the loop, follow us on LinkedIn to catch fresh articles every week.</a></p>

                <p style="color: #000000; text-align: justify;">This blog was inspired by <a href="https://www.linkedin.com/in/jeandjoseph/">Jean Joseph</a>, a Data & AI driven professional with over tweenty years of experience helping organizations unlock insights through analytics and AI. If you are looking for <a href="https://www.techinsightgroup.com/svc-consulting.html">consulting</a> and <a href="https://www.techinsightgroup.com/svc-1day-training.html">training</a> services, please reach out to the <a href="https://www.techinsightgroup.com/index.html">Tech-Insight-Group LLC</a> team</a>.</p>

                                    
                    
                <h5 class="text-primary mt-4">Prerequisites for Completing This Hands‑On Exercise</h5>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b><a href="https://app.fabric.microsoft.com/">Access to a Microsoft Fabric Tenant:</a></b> You must have access to a Microsoft Fabric-enabled tenant. If your organization has not enabled Fabric yet, contact your administrator to activate it.</li>
                    <li style="color: #000000; text-align: justify;"><b>A Provisioned <a href="https://learn.microsoft.com/en-us/fabric/enterprise/licenses">Fabric Capacity:</a></b> To run notebooks, Spark workloads, and Lakehouse operations, you need a Fabric capacity (F‑SKU or P‑SKU, trial capacity).</li>
                    <li style="color: #000000; text-align: justify;"><b>A <a href="https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-lakehouse-get-started">Lakehouse</a> in Your Workspace:</b> Create a new Lakehouse where you will store and explore the dataset. This Lakehouse will serve as the foundation for all EDA steps in this article.</li>
                    <li style="color: #000000; text-align: justify;"><b><a href="https://github.com/jeandjoseph/community/blob/main/AI/ms-fabric/csv/regression_housing.csv">Download and Upload the Dataset:</a></b> Download the regression_housing.csv file provided with this article. Then upload it into the Files section of your Lakehouse so it can be accessed by your notebook.</li>
                    <li style="color: #000000; text-align: justify;"><b>A <a href="https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook">Fabric Notebook:</a></b> Create a new Fabric Notebook inside the same workspace. You will run all the code snippets from this article inside that notebook using the built‑in Spark runtime.</li>
                    <li style="color: #000000; text-align: justify;"><b>Once these prerequisites are complete</b>, you’re ready to follow the hands‑on instructions to perform Model Training and Model Evaluation in Part 3, which will prepare you to deploy the ML model and test it using batch scoring in Part 4.</li>
                </ul>  

                <h5 class="text-primary mt-4">Who is this article for?</h5>
                <p style="color: #000000; text-align: justify;">These series of article are designed for data professionals, business leaders, and technical teams who want to successfully implement machine learning projects that deliver real business value. Whether you are a data scientist, ML engineer, or decision-maker exploring AI adoption, you’ll benefit from learning a proven, end-to-end strategy from framing the right problem to building and deploying scalable solutions in <a href="https://www.microsoft.com/en-us/microsoft-fabric/resources/data-101/what-is-fabric?msockid=1215ff17aded6042147ae933acea6173">Microsoft Fabric</a>.</p>

                
                <p style="color: #000000; text-align: justify;">This is a four-part article, and you are currently reading the <b>Second Part</b>. What is the objective here? It’s to share and remind you how to properly tackle a machine learning (ML) project from the ground up. </p>


                <p style="color: #000000; text-align: justify;">Success in ML doesn’t come from algorithms alone, it depends on how the entire pipeline is designed and aligned with business goals. Every step in the ML lifecycle, from data ingestion to deployment, has direct implications for cost efficiency, scalability, and customer trust. If the foundation is weak, even the most advanced models will fail to deliver meaningful results.</p>

                <p style="color: #000000; text-align: justify;">Our goal is to guide you through an end-to-end process that starts with identifying and framing the right problem, then leveraging <a href="https://learn.microsoft.com/en-us/fabric/data-science/machine-learning-model">Microsoft Fabric for development and implementation</a>.</p>

 
                <h5 class="text-primary mt-4">Introduction</h5>

                <p style="color: #000000; text-align: justify;">This is the <b>Second Part</b> of this series of articles. In <a href="blg-fabric-ml-success-strategies-from-start-2-finish-part-1.html">Part 1</a>, we explained how to frame your machine‑learning problem, validate that ML is the right tool, and determine which type of model to use once the problem is clearly defined. With the problem properly scoped, and the ML approach identified, the next essential step is understanding the data that will power your solution.</p>
                
                <p style="color: #000000; text-align: justify;"><b>In this Part 2</b>, we focus on <a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory Data Analysis (EDA)</a>, one of the most critical, yet often overlooked, phases of any ML workflow. <i>EDA helps you uncover patterns, distributions, anomalies, biases, and relationships within your dataset before you ever train a model.</i></p>
                
                <p style="color: #000000; text-align: justify;">By visualizing and summarizing the data, you gain early insight into data quality issues, feature behavior, and potential signals that drive model performance. Ultimately, a solid EDA foundation leads to better features, stronger models, and more confident decision‑making throughout the ML lifecycle.</p>
                

                <h5 class="text-primary mt-4">Dataset introduction (short narrative)</h5>
                <p style="color: #000000; text-align: justify;">First, let us understand our dataset. <a href="https://github.com/jeandjoseph/community/blob/main/AI/ms-fabric/csv/regression_housing.csv">regression_housing.csv</a> file contains residential property records used to model <b>sale price</b> (target) from property and location attributes (features). Typical columns include <b>city, neighborhood, beds, baths, sqft, built_year, lot_acres, has_garage, has_renovation, and price</b>. <i>The goal is to understand the distribution, quality, and signal in these variables, especially how price varies by location and property characteristics, before feature engineering or modeling.</i></p>

                <p style="color: #000000; text-align: justify;">Let us start exploring the dataset by loading the <b>regression_housing.csv</b> file into a <b>Spark DataFrame</b>, automatically detecting column types and treating empty values as nulls, so the data is ready for cleanup and analysis.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# Replace the path with your file location in the Lakehouse:
csv_path = "Files/csv/ml/regression_housing.csv"

# Read with header, infer schema; we'll fix types afterwards to be safe.
raw_data_df = (
    spark.read
         .option("header", "true")
         .option("inferSchema", "true")
         # Treat empty strings as nulls to simplify downstream casting
         .option("nullValue", "")
         .csv(csv_path)
)

display(raw_data_df.limit(5))
</code></pre>


                <p style="color: #000000; text-align: justify;">This below screenshot shows the first five rows of the housing dataset, <b>confirming that key fields, such as city, neighborhood, beds, baths, square footage, build year, lot size, garage/renovation indicators, and price were correctly loaded and parsed into the DataFrame</b>.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_1.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <p style="color: #000000; text-align: justify;">Now that we’ve loaded the contents of the CSV file into a <b>Spark DataFrame</b>, it’s important to understand the structure of the data before moving further. Inspecting the schema with <b>raw_data_df.printSchema()</b> allows us to verify that each column has been read with the correct data type and ensures that numerical, categorical, and boolean fields are properly interpreted. This early check helps prevent downstream issues in cleaning, feature engineering, and modeling, making it a best practice in any EDA workflow.</p>                

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# Pretty tree view of the schema
raw_data_df.printSchema()
</code></pre>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_2.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <h5 class="text-primary mt-4">Quick completeness check (missing values per column)</h5>

                <p style="color: #000000; text-align: justify;">Knowing the <b>content</b> and <b>schema definition</b> of the CSV file, the next step is to <b>generate descriptive statistics to understand how the numeric features behave overall using summary()</b>. <i>By reviewing minimum, maximum, mean, and standard deviation values, we can quickly validate whether ranges look reasonable for example, square footage typically falls within a few hundred to several thousand square feet, while lot acreage is usually small and often skewed</i>.</p>


<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# raw_data_df.describe().show()

# More robust summary (includes percentiles for numeric cols)
display(raw_data_df.summary())
</code></pre>


                <p style="color: #000000; text-align: justify;"><b>Below screenshot is the output of the above scripts, it clearly shows missing data when we examine the count statistics for each feature:</b> columns like baths, sqft, and lot_acres report a count of <b>970 instead</b> of the full <b>1000 rows</b>, indicating 30 nulls in each, whereas other columns (e.g., beds, built_year, price) show complete counts, a signal that these partially missing fields need imputation or special handling before modeling.</p>

                <p style="color: #000000; text-align: justify;">This output also helps reveal important characteristics such as the spread of built_year and the presence of heavy tails in the price distribution, which are critical to recognize before moving deeper into EDA or modeling.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_3.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <p style="color: #000000; text-align: justify;">Next, perform a quick completeness check to validate the missing‑value information observed in the <b>summary()</b> output using the below scripts.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
from pyspark.sql import functions as F

# Get all column names from the raw dataset
cols = raw_data_df.columns

# Compute missing value count for each column
missing_counts = raw_data_df.select([
    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in cols
])

# Display missing value summary as a single-row table
display(missing_counts)
</code></pre>


                <p style="color: #000000; text-align: justify;">This completeness check confirms our earlier findings: the columns baths, sqft, and lot_acres each contain exactly 30 missing values, while all other fields including the target price have zero missing entries. This validates that only these three features require imputation, and the rest of the dataset is fully complete.</p>



                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_4.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <h5 class="text-primary mt-4">Handling Missing Values (Imputation Step)</h5>

                <p style="color: #000000; text-align: justify;">In a real project, best‑practice missing‑value handling would involve more robust strategies such as <b>median imputation</b>, <b>distribution‑aware</b> methods, or even model‑based techniques, especially for skewed numeric features. However, because this is a small synthetic dataset used purely for demonstration, we will keep things simple and rely on <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html">approxQuantile</a> and <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html">mean‑based</a> <b>imputation</b> to fill the missing values.</p>

                <p style="color: #000000; text-align: justify;">Here is what we are doing in the script below. We cast sqft, lot_acres, and baths to numeric and impute with <b>medians (via approxQuantile) because medians are robust to skew and outliers, giving stable fills for right‑tailed housing features</b>. <i>Using approxQuantile is efficient at scale and delivers tight estimates without collecting the entire column.</i></p>

                <p style="color: #000000; text-align: justify;">We add a <b>fallback to mean (or 0)</b> only if a column is entirely null, ensuring the pipeline never breaks. Completing imputation to reach <b>full counts across columns avoids bias, data loss</b> from row drops, and model failures from NaNs, making the dataset consistent and ready for reliable feature engineering and ML training.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
from pyspark.sql import functions as F

# Columns to impute
target_cols = ["sqft", "lot_acres", "baths"]

# 1) Cast target columns to numeric (DoubleType) if needed
df_num = raw_data_df.select([
    F.col(c).cast("double").alias(c) if c in target_cols else F.col(c)
    for c in raw_data_df.columns
])

# 2) Compute medians using approxQuantile
#    relativeError=0.001 gives a tight approximation while staying efficient
medians = {}
for c in target_cols:
    # Handle the case where column is all nulls (approxQuantile returns empty)
    quantiles = df_num.stat.approxQuantile(c, [0.5], 0.001)
    medians[c] = quantiles[0] if quantiles else None

# Optional: if a column has no non-null values, fall back to 0 or mean
fallbacks = {}
for c in target_cols:
    if medians[c] is None:
        # fallback to mean; if mean is also None, fallback to 0
        mean_val = df_num.select(F.mean(F.col(c))).first()[0]
        fallbacks[c] = mean_val if mean_val is not None else 0.0

# Combine medians with fallbacks
fill_values = {c: (medians[c] if medians[c] is not None else fallbacks[c]) for c in target_cols}

# 3) Impute nulls
df_imputed = df_num.na.fill(fill_values)

# Preview results
display(df_imputed.summary())  # quick sanity check
</code></pre>

                <p style="color: #000000; text-align: justify;">Notice that after applying our imputation steps, the summary output now shows a count of <b>1000 for every column</b>, including <b>sqft, baths, and lot_acres</b> which previously had missing values. This confirms that all null entries were successfully filled using the median (with fallbacks where needed), resulting in a fully complete dataset with no gaps. This is an essential prerequisite before moving into deeper EDA, feature engineering, or modeling.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_5.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                <h5 class="text-primary mt-4">Uncovering Distribution Patterns to Understand Data Structure</h5>

                <p style="color: #000000; text-align: justify;">Uncovering distribution patterns helps reveal the underlying structure of the data, allowing us to <b>identify skewness, outliers, and variability</b> early so that downstream analyses and models are built on a more accurate understanding of how the data truly behaves.</p>

                <p style="color: #000000; text-align: justify;"><b>Median and IQR Analysis for Price Variation Across Locations</b></p>

                <p style="color: #000000; text-align: justify;">These <b>scripts compute median price and price spread (IQR) for each city and neighborhood</b> so you can clearly compare which markets are most expensive, which are cheapest, and which have the most variability in home prices.</p>


<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
from pyspark.sql import functions as F

# Median (p50) and IQR per city
city_stats = df_imputed.groupBy("city").agg(
    F.expr("percentile(price, 0.5)").alias("median_price"),
    F.expr("percentile(price, 0.25)").alias("p25_price"),
    F.expr("percentile(price, 0.75)").alias("p75_price"),
    F.count("*").alias("n")
).withColumn("IQR_price", F.col("p75_price") - F.col("p25_price")) \
 .orderBy(F.desc("median_price"))
display(city_stats)

# Median (p50) and IQR per neighborhood
nhood_stats = df_imputed.groupBy("neighborhood").agg(
    F.expr("percentile(price, 0.5)").alias("median_price"),
    F.expr("percentile(price, 0.25)").alias("p25_price"),
    F.expr("percentile(price, 0.75)").alias("p75_price"),
    F.count("*").alias("n")
).withColumn("IQR_price", F.col("p75_price") - F.col("p25_price")) \
 .orderBy(F.desc("median_price"))
display(nhood_stats)
</code></pre>




                <p style="color: #000000; text-align: justify;"><b>What the two below outputs from the above scripts are telling you (summary)</b></p>

                <p style="color: #000000; text-align: justify;"><b>Cities (first output):</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Jacksonville and Idaho Falls</b> have the highest median prices, meaning they are the most expensive markets in your dataset.</li>
                    <li style="color: #000000; text-align: justify;">Cities like <b>Arlington and Boston</b> have lower medians.</li>
                    <li style="color: #000000; text-align: justify;">IQR values differ widely, showing that some cities have more <b>price variability</b> than others.</li>
                </ul>       
                
                
                <p style="color: #000000; text-align: justify;"><b>Neighborhoods(second output):</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Downtown</b> has the <b>highest median and the largest IQR</b>, confirming a premium, diverse-price market.</li>
                    <li style="color: #000000; text-align: justify;"><b>Suburban</b> is mid‑priced.</li>
                    <li style="color: #000000; text-align: justify;"><b>Rural</b> is the least expensive with the smallest price spread.</li>
                </ul>                   

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_6.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

               <h5 class="text-primary mt-4">Finding Robust Outliers in Pricing and Property Metrics</h5>

                <p style="color: #000000; text-align: justify;">Now that you have a solid understanding of <b>Median and IQR</b> Analysis for Price Variation Across Locations, it’s important to recognize that this approach is also ideal for detecting outliers. </p>
                <p style="color: #000000; text-align: justify;"><b>Robust Outlier Detection for Pricing and Property Metrics provides essential insight</b> into extreme or unusual values that may distort analysis, bias models, or hide meaningful trends in the data. Using statistically grounded methods such as the IQR rule, this technique identifies listings with abnormally high prices, unusually large lot sizes, or atypical property characteristics that could mislead averages, skew correlations, or weaken predictive models. </p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
def iqr_bounds(col):
    # Compute Q1 and Q3 using approximate quantiles
    q = df_imputed.approxQuantile(col, [0.25,0.75], 0.001)
    Q1, Q3 = q[0], q[1]

    # Calculate IQR and standard outlier bounds    
    IQR = Q3 - Q1
    lower = Q1 - 1.5*IQR
    upper = Q3 + 1.5*IQR
    return lower, upper

rows = []
for c in ["price","sqft","lot_acres","baths","beds"]:
    # Get lower/upper bounds for each numeric column
    lower, upper = iqr_bounds(c)
    # Count outliers below and above the IQR thresholds
    cnt_low = df_imputed.filter(F.col(c) < lower).count()
    cnt_high = df_imputed.filter(F.col(c) > upper).count()
    # Store results for summary table
    rows.append((c, lower, upper, cnt_low, cnt_high))

# Create summary DataFrame of outlier counts per column
out_df = spark.createDataFrame(rows, ["column","lower_bound","upper_bound","num_below","num_above"])

# Display columns sorted by highest number of high-end outliers
display(out_df.orderBy(F.desc("num_above")))
</code></pre>


                <p style="color: #000000; text-align: justify;">As shown in the output below from the above sripts completion, Median and IQR Analysis helps detect outliers by calculating robust lower and upper bounds for each feature and identifying how many values fall outside them. This reveals extremely large lot sizes, high-end prices, and unusually large homes that could distort averages and weaken predictive models.</p>

                <p style="color: #000000; text-align: justify;"><b>Method recap:</b> Outliers are values < Q1 − 1.5×IQR (lower bound) or > Q3 + 1.5×IQR (upper bound).</p>


                <p style="color: #000000; text-align: justify;"><b>1) lot_acres:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Bounds:</b> lower ≈ <b>−0.2225</b> (not meaningful for acres), upper ≈ <b>1.3815</b></li>
                    <li style="color: #000000; text-align: justify;"><b>Outliers:</b> <b>58 above</b>, <b>0</b> below</li>
                    <li style="color: #000000; text-align: justify;"><b>Meaning:</b> Many properties have lot <b>sizes > 1.3815 acres</b>, indicating a <b>long right tail</b> (large parcels). These can skew averages and may represent luxury/rural listings.</li>
                </ul>  

                <p style="color: #000000; text-align: justify;"><b>2) price:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Bounds:</b> lower ≈ <b>−69,514.5</b> (not meaningful), upper ≈ <b>1,142,185.5</b></li>
                    <li style="color: #000000; text-align: justify;"><b>Outliers:</b> <b>16</b> above, <b>0</b> below</li>
                    <li style="color: #000000; text-align: justify;"><b>Meaning:</b> There are <b>16 high‑priced listings (> $1.14M)</b>. Price is <b>right‑skewed</b> with a high‑end segment; these can bias means/correlations and affect model residuals.</li>
                </ul>  

                <p style="color: #000000; text-align: justify;"><b>3) sqft:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Bounds:</b> lower ≈ <b>260.5</b>, upper ≈ <b>3,344.5</b></li>
                    <li style="color: #000000; text-align: justify;"><b>Outliers:</b> <b>4</b> above, <b>0</b> below</li>
                    <li style="color: #000000; text-align: justify;"><b>Meaning:</b> A few homes <b>exceed 3,344.5 sqft</b> (large properties). Smaller than <b>260.5 sqft</b> is rare/impossible here, hence only high outliers.</li>
                </ul>  

                <p style="color: #000000; text-align: justify;"><b>4) beds:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Bounds:</b> lower ≈ <b>−1.0</b>, upper ≈ <b>7.0</b></li>
                    <li style="color: #000000; text-align: justify;"><b>Outliers:</b> <b>0</b> above, <b>0</b> below</li>
                    <li style="color: #000000; text-align: justify;"><b>Meaning:</b> Bed counts fall within <b>0–7</b>; no outliers by IQR. <b>Distribution is well‑contained</b>.</li>
                </ul>  

                <p style="color: #000000; text-align: justify;"><b>5) baths:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Bounds:</b> lower ≈ <b>−2.0</b>, upper ≈ <b>6.0</b></li>
                    <li style="color: #000000; text-align: justify;"><b>Outliers:</b> <b>0</b> above, <b>0</b> below</li>
                    <li style="color: #000000; text-align: justify;"><b>Meaning:</b> Bath counts are within <b>0–6</b>; no outliers detected. Stable, <b>discrete distribution</b>.</li>
                </ul>  
                                                     

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_7.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <p style="color: #000000; text-align: justify;">To summarize, detecting these outliers early ensures cleaner data, more stable model performance, and a clearer understanding of true market behavior, ultimately enabling more accurate pricing insights and more reliable decision‑making.</p>


                <h5 class="text-primary mt-4">Frequency tables</h5>
                <p style="color: #000000; text-align: justify;"><b>What the frequency tables imply for these plots</b></p>

                <p style="color: #000000; text-align: justify;"><b>Frequency tables are essential because they show how often each unique value appears in a categorical or discrete feature, helping you quickly understand the distribution, dominance, and balance of categories in the dataset</b>. They make it easy to spot issues such as rare categories, class imbalance, unusual patterns, or unexpected entries, all of which directly impact feature engineering, encoding decisions, sampling strategies, and model performance.</p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;">If your frequency tables show many more <b>Downtown</b> than <b>Rural</b> homes (or vice versa), it <b>affects:Stability:</b> More records → more reliable box <b>estimates.Outlier density:</b> Small groups can show outliers more prominently because each point represents a larger fraction.</li>
                </ul>                  
               

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# Frequency tables for key categorical columns
for c in ["city","neighborhood","has_garage","has_renovation"]:
    # Compute value counts sorted by highest frequency
    freq = df_imputed.groupBy(c).count().orderBy(F.desc("count"))
    # Display distribution for each column
    display(freq)
</code></pre>

                <p style="color: #000000; text-align: justify;"><b>For has_garage / has_renovation:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;">If most homes have a garage, then garage-related price differences may be subtle in global plots. Better to stratify (e.g., price by neighborhood and garage presence).</li>
                    <li style="color: #000000; text-align: justify;">If renovations are rare, outliers in price for renovated homes will stand out, but conclusions should be cautious.</li>
                </ul>                  
               
                <p style="color: #000000; text-align: justify;"><b>Why it matters:</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;">It tells you how many records come from each city and neighborhood.</li>
                    <li style="color: #000000; text-align: justify;">It shows the class balance for has_garage and has_renovation (e.g., many houses with garages vs. without).</li>
                    <li style="color: #000000; text-align: justify;">Any imbalance affects how you interpret the boxplots and how reliable comparisons are. If one city or neighborhood has only a handful of homes, its distribution will be noisier.</li>
                </ul>        

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_8.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                
                
                 <h5 class="text-primary mt-4">Price by city/neighborhood (boxplot via pandas for visualization)</h5>

                 <p style="color: #000000; text-align: justify;">When it comes to outliers, one of the best ways to quickly spot areas with unusually high or inconsistent values is to rely on boxplots, because they clearly show the median, IQR range, and any points that fall outside the whiskers.</p>           


<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# Price by city/neighborhood (boxplot via pandas for visualization)
# Sample 20% of data and convert to pandas for plotting
pdf = df_imputed.select("city","neighborhood","price").sample(0.2, seed=7).toPandas()

import seaborn as sns, matplotlib.pyplot as plt
# Boxplot: price distribution across cities
fig, ax = plt.subplots(figsize=(9,5))
sns.boxplot(data=pdf, x="city", y="price")
ax.set_title("Price distribution by city")
display(fig)

# Boxplot: price distribution across neighborhoods
fig, ax = plt.subplots(figsize=(10,5))
sns.boxplot(data=pdf, x="neighborhood", y="price")
ax.set_title("Price distribution by neighborhood")
ax.tick_params(axis='x', rotation=30)
display(fig)
</code></pre>



                <p style="color: #000000; text-align: justify;"><b>What do the below figures produced by the above scripts show?</b></p>

                <p style="color: #000000; text-align: justify;"><b>A) Price distribution by city (top & third plots):</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;">Each boxplot summarizes <b>median price (center line), IQR (box), range without outliers (whiskers), and outliers (dots)</b> for each city.</li>
                    <li style="color: #000000; text-align: justify;">You can quickly compare cities: Some cities (e.g., those with taller boxes and higher medians) show <b>higher typical prices and greater variability</b>. Cities with more dots above the whiskers have <b>more high-end outliers</b> (luxury listings or very large properties).
                </ul>  

                <p style="color: #000000; text-align: justify;"><b>B) Price distribution by neighborhood (second & bottom plots)</b></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><b>Downtown</b> shows the <b>highest median and widest spread</b>, indicating <b>expensive and variable pricing</b>.</li>
                    <li style="color: #000000; text-align: justify;"><b>Suburban</b> has a <b>moderate median</b> with a <b>few high-price outliers</b>.</li>
                    <li style="color: #000000; text-align: justify;"><b>Rural</b> displays the <b>lowest median and tightest spread</b>, suggesting <b>more consistent, lower pricing</b>.</li>
                </ul>  

                <p style="color: #000000; text-align: justify;"><i>The duplicated city and neighborhood plots appear because the script generates and displays two separate figures for each (likely re-run cells or intended to compare different renders).</i></p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_9.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <p style="color: #000000; text-align: justify;">To summarize, there are many techniques for detecting and handling outliers, but for the sake of this demo we won’t dive deeper into them and will instead shift our focus toward understanding how the features relate to one another.</p>

                
                <h5 class="text-primary mt-4">Analyzing Feature Relationships Through Correlation</h5>

                <p style="color: #000000; text-align: justify;">In <b>supervised machine learning</b>, relationships between features matter because models learn patterns from how variables interact and influence the target. Understanding these relationships early in EDA helps reveal which features move together, which ones may add redundant information, and which truly carry predictive signal.</p>

                <p style="color: #000000; text-align: justify;">To explore these interactions, we’ll use the .corr() method to compute correlation values and gain a clearer view of how our numerical features relate to one another before moving into modeling using below scripts.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
# Simple Pandas/Seaborn approach (for small datasets)
import seaborn as sns
import matplotlib.pyplot as plt

# Convert selected numeric and binary columns to pandas for correlation analysis
pdf = (df_imputed
       .select("beds", "baths", "sqft", "built_year", "lot_acres",
               "has_garage", "has_renovation", "price")
       .toPandas())

# Compute Pearson correlation matrix       
corr_mat = pdf.corr(method="pearson")  # DataFrame of correlations

# Plot heatmap of correlations
plt.figure(figsize=(10, 8))
sns.set(style="white", font_scale=1.0)
sns.heatmap(
    corr_mat,
    annot=True,
    fmt=".2f",
    cmap="coolwarm",
    vmin=-1, vmax=1,
    square=True,
    linewidths=.5
)
plt.title("Correlation Heatmap (Pearson)")
plt.tight_layout()
plt.show()
</code></pre>

                <p style="color: #000000; text-align: justify;"><B>How to Read below Heatmap output produced by aboce codes (Simple Explanation)</B></p>

                <p style="color: #000000; text-align: justify;"><B>Takeaway:</B> These codes will generate a heatmap shows that sqft is the most important numeric driver of price here, while other features have very small linear relationships.</p>

                <p style="color: #000000; text-align: justify;"><B>Each cell shows how strongly two variables move together. Values range from –1 to +1:</B></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><B>Red</B> = positive correlation: when one goes up, the other tends to go up.</li>
                    <li style="color: #000000; text-align: justify;"><B>Blue</B> = negative correlation: when one goes up, the other tends to go down.</li>
                    <li style="color: #000000; text-align: justify;"><B>White/gray</B> = little or no relationship.</li>
                </ul>                  
 
                <p style="color: #000000; text-align: justify;"><B>Key Insights From This Heatmap:</B></p>
                <ul class="mb-4">
                    <li style="color: #000000; text-align: justify;"><B>Square footage (sqft)</B> has the strongest positive correlation with price (~0.33). → Bigger homes tend to be more expensive.</li>
                    <li style="color: #000000; text-align: justify;"><B>Beds, baths, lot size, built year, garage, renovation </B> show weak correlations with price (near 0). → In this dataset, they do not strongly influence price on their own.</li>
                    <li style="color: #000000; text-align: justify;"><B>Diagonal</B> = 1.0 because every variable is perfectly correlated with itself.</li>
                </ul>       

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_10.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                <p style="color: #000000; text-align: justify;">So far in our workflow, we’ve successfully ingested the raw dataset, performed the essential cleaning steps, and carried out a structured exploratory data analysis. We examined variable distributions, identified and quantified outliers using robust IQR‑based methods, and explored how different features relate to one another through correlation analysis and visualizations like boxplots. </p>

                <p style="color: #000000; text-align: justify;">With a solid understanding of the data’s structure, quality, and relationships, we’re now ready to move into the next critical phase of the machine learning pipeline: feature engineering, where we’ll transform and enhance the dataset to make it more suitable and more powerful for model training.</p>              
                

                <h5 class="text-primary mt-4">Feature Engineering: Preparing Data for Machine Learning Success</h5>

                <p style="color: #000000; text-align: justify;"><B>Feature engineering</B> is essential because it transforms raw data into meaningful, model‑ready inputs, helping machine learning algorithms capture the right patterns, reduce noise, and ultimately deliver more accurate and reliable predictions.</p>

                <p style="color: #000000; text-align: justify;">If you recall, our dataset contains categorical features such as <B>city and neighborhood</B>, and since machine learning models cannot work directly with text-based categories, we need to convert these fields into numerical format through <B>feature engineering</B> techniques like <B>one‑hot encoding</B>.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_11.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                <p style="color: #000000; text-align: justify;"><B>What is One-Hot enconding?</B></p>

                <p style="color: #000000; text-align: justify;"><A href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">One‑hot</a> encoding is a method that converts each categorical value into a separate binary column, marking 1 for the matching category and 0 for all others. Let us use below codes to proceed.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

cat_cols = ["city", "neighborhood"]

# Extract only categorical columns to pandas (efficient for encoding)
pdf_cat = df_imputed.select(cat_cols).toPandas()

# Fit OneHotEncoder and transform categorical values into binary columns
enc = OneHotEncoder(handle_unknown="ignore", sparse_output=False)  # use sparse_output=True for large data
X_cat = enc.fit_transform(pdf_cat)
oh_cols = enc.get_feature_names_out(cat_cols)

# If you want the full encoded pandas DataFrame:
# Convert full Spark DataFrame to pandas for merging encoded features
pdf_full = df_imputed.toPandas()

# Build final DataFrame: drop original categorical columns, add encoded ones
df_oh = pd.concat(
    [pdf_full.drop(columns=cat_cols).reset_index(drop=True),
     pd.DataFrame(X_cat, columns=oh_cols)],
    axis=1
)

# Compare feature counts before and after encoding
print("Original columns:", len(pdf_full.columns))
print("One-hot encoded columns:", len(df_oh.columns))

# Preview encoded dataset
display(df_oh.head(5))
</code></pre>


                <p style="color: #000000; text-align: justify;">Thank you for the great work, your progress is clear and genuinely appreciated. As you continue, keep in mind that <b>one-hot encoding</b> can significantly expand the feature space, as shown in the image: <b>the original dataset had 10 columns, but after encoding, it grew to 21</b>.</p>

                <p style="color: #000000; text-align: justify;">Too many features can hurt model performance, so one-hot encoding should be applied thoughtfully. There are other encoding techniques that can achieve similar results with fewer dimensions, such as target encoding or embeddings but we won’t cover those here. Staying mindful of feature volume will help maintain accuracy, efficiency, and scalability.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_12.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                <p style="color: #000000; text-align: justify;">Take a look at the same dataset, even though we’ve completed one-hot encoding, there’s still more work to be done. Several features, such as square footage, lot size, and year built, vary widely in scale and units. Without proper scaling, these differences can distort model behavior and reduce performance. That said, let’s now deep dive into feature scaling to ensure our inputs are balanced and ready for modeling.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_13.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>


                <h5 class="text-primary mt-4">Feature Scaling & Standardization</h5>

                <p style="color: #000000; text-align: justify;"><b>Why do we need feature scaling?</b></p>

                <p style="color: #000000; text-align: justify;"><a href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">Feature scaling</a> ensures that all input features contribute equally to the model by putting them on a similar scale. Without it, features with larger ranges (like square footage or lot size) can dominate distance-based models or slow down gradient-based optimization. Scaling improves model accuracy, convergence speed, and interpretability.</p>

                <p style="color: #000000; text-align: justify;">Let us execute these below codes and display the first top five rows</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
import numpy as np
from sklearn.preprocessing import StandardScaler

target_col = "price"  # label
feature_cols = [c for c in df_oh.columns if c != target_col]

# Numeric columns to scale (skip one-hot 0/1 by nunique==2)
# Identify numeric columns to scale (exclude one-hot encoded 0/1 columns)
num_cols = [
    c for c in feature_cols
    if np.issubdtype(df_oh[c].dtype, np.number) and df_oh[c].nunique() > 2
]
oh_cols = [c for c in feature_cols if c not in num_cols]  # keep as-is

# Fit scaler on TRAIN ONLY in a real pipeline; for demo we fit on all
# Fit scaler (note: in production, fit only on TRAIN split)
scaler = StandardScaler(with_mean=True, with_std=True)
scaler.fit(df_oh[num_cols])

# Create the final scaled dataframe
# Apply scaling to numeric columns
df_scaled = df_oh.copy()
df_scaled[num_cols] = scaler.transform(df_scaled[num_cols])

# Preview scaled dataset
display(df_scaled.head(5))
</code></pre>

                <p style="color: #000000; text-align: justify;">The output below shows that only the numeric features were standardized using above scripts, which is why columns like <b>beds, baths, sqft, built_year, and lot_acres now appear as scaled values centered around zero</b>. As a result, the dataset looks much more balanced, the wide‑range numeric features are now on a similar scale, while the <b>one‑hot encoded city columns remain as simple 0/1 indicators</b>. This is exactly the behavior we expect from the script.</p>

                <p style="color: #000000; text-align: justify;">Therefore, the dataset is not a problem, we intentionally standardized only the numeric features that needed scaling, and <b>left the one‑hot encoded columns untouched because scaling them would distort their meaning</b>.</p>

                <p style="color: #000000; text-align: justify;"></p>
                <img src="img/fb_ml_eda_14.png" class="img-fluid w-100 rounded" alt=""/>
                <p style="color: #000000; text-align: justify;"></p>

                <p style="color: #000000; text-align: justify;">Now that our dataset has been fully cleaned, encoded, and scaled, we can save this final version for the next stage of our workflow. The script below writes the processed dataset to a dedicated output folder, ensuring it’s neatly organized and ready for reuse. With this cleaned dataset stored, we’re all set to move on to model training, which we’ll cover in Part 3.</p>

<style>
.code-block {
  background-color: #000;       /* Pure black background */
  color: #fff;                  /* White text */
  padding: 1em;
  border-radius: 5px;
  font-family: monospace;
  overflow-x: auto;
  white-space: pre-wrap;
  box-shadow: none;            /* Removes any default shadow */
}
</style>

<pre class="code-block"><code>
import os

# Output directory for the cleaned, fully prepared dataset
out_dir = "Files/csv/ml/cleaned"
os.makedirs(out_dir, exist_ok=True)  # create all missing parent folders

# Build full file path and save the scaled dataset
out_path = os.path.join(out_dir, "regression_housing_cleaned.csv")
df_scaled.to_csv(out_path, index=False)

# Confirm save location
print(f"Saved to: {out_path}")
</code></pre>



                <h5 class="text-primary mt-4">Summarization</h5>

                <p style="color: #000000; text-align: justify;">Today you completed a full round of data preparation for your housing regression project. You started with exploratory data analysis (EDA) to understand the structure, distributions, and potential issues in the raw dataset. Then you cleaned the data, applied one‑hot encoding to convert categorical features into machine‑readable form, and standardized the numeric columns to bring them onto a more balanced scale. </p>
                <p style="color: #000000; text-align: justify;">If you’re not very comfortable writing code, you can rely on <a href="https://learn.microsoft.com/en-us/fabric/data-science/data-wrangler">Fabric Data Wrangler</a> to handle much of the early EDA and data preparation for you. It provides an intuitive, no‑code interface for profiling your dataset, detecting missing values, previewing distributions, and applying common cleaning steps before moving into more advanced analysis.</p>
                <p style="color: #000000; text-align: justify;">Along the way, you inspected the transformed outputs, verified that the dataset now looks more consistent and model‑ready, and confirmed why only numeric features not one‑hot columns should be scaled. Finally, you saved the fully processed dataset so it’s ready to be used in the next phase.</p>
                <p style="color: #000000; text-align: justify;">With this foundation in place, you’re now set up for <a href="blg-fabric-ml-success-strategies-from-start-2-finish-part-3.html">Part-3</a>, where you’ll move into model training using this cleaned, explored, and standardized dataset. </p>    
                  
                <h3 class="text-primary mt-4">Call To Action</h3>                 
                

                <h5 class="text-primary mt-4">💡 Ready to Take the Next Step?</h5>  

                <p style="color: #000000; text-align: justify;">If you’re reading this to kick off your first AI, Data project, streamline your current workflow, or upskill your team for what’s next. <b>Tech-Insight-Group LLC</b> is here to help. We specialize in turning AI potential into practical impact through expert <a href="https://www.techinsightgroup.com/svc-consulting.html">consulting</a> and hands-on <a href="https://www.techinsightgroup.com/svc-1day-training.html">training</a>. Visit our services page to explore how we can support your journey from strategy to execution. Let’s build something extraordinary together.</p>               
                
                
                <h5 class="text-primary mt-4">🙏 We welcome your feedback, let’s connect.</h5>  
 
                <p style="color: #000000; text-align: justify;">Thank you for reading <b>Fabric ML Lifecycle: Success Strategies from Start to Finish – Part 2: Focusing on Exploratory Data Analysis (EDA) & Pre‑Processing</b>. If you found this article helpful, feel free to like, share, or leave a comment; we’d love to hear your thoughts.</p>
                <p style="color: #000000; text-align: justify;">Kudos to our entire team for their dedication, and a special shoutout to <a href="https://www.linkedin.com/in/jeandjoseph/"> Jean Joseph</a>, our <b>Principal Data & AI Architect</b>, whose vision and technical leadership made this work possible.</p>
   
                
                <br><br>
              <!-- Area where we can share -->

                <div class="col-lg-6 col-xl-4 wow fadeIn" data-wow-delay=".3s">
                    <div class="blog-item position-relative bg-light rounded">
                        <img src="img/fb_ml_eda_0.png" class="img-fluid w-100 rounded-top" alt="">
                        <span class="position-absolute px-4 py-3 bg-primary text-white rounded" style="top: -28px; right: 20px;">Fabric ML Lifecycle: Success Strategies from Start to Finish – Part 2: Focusing on Exploratory Data Analysis (EDA) & Pre‑Processing</span>
                        <div class="blog-btn d-flex justify-content-between position-relative px-3" style="margin-top: -75px;">
                            <div class="blog-btn-icon btn btn-secondary px-4 py-3 rounded-pill ">
                                <div class="blog-icon-1">
                                    <p class="text-white px-2">Share<i class="fa fa-arrow-right ms-3"></i></p>
                                </div>
                                <div class="blog-icon-2">
                                <a href="https://www.facebook.com/sharer/sharer.php?u=https://www.techinsightgroup.com/blg-fabric-ml-success-strategies-from-start-2-finish-part-2.html" target="_blank" class="btn btn-social me-1">
                                    <i class="fab fa-facebook-f text-white"></i>
                                </a>
                                <a href="https://twitter.com/intent/tweet?url=https://www.techinsightgroup.com/blg-fabric-ml-success-strategies-from-start-2-finish-part-2.html" target="_blank" class="btn btn-social me-1">
                                    <i class="fab fa-twitter text-white"></i>
                                </a>
                                <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.techinsightgroup.com/blg-fabric-ml-success-strategies-from-start-2-finish-part-2.html" target="_blank" class="btn btn-social me-1">
                                    <i class="fab fa-linkedin-in text-white"></i>
                                </a>
                                </div>
                            </div>
                        </div>

                    </div>
                </div>

                <br><br>
              
                <div style="margin-top: 40px;">
                    <a href="blg-fabric-ml-success-strategies-from-start-2-finish-part-1.html">Part 1 ← | → </a> 

                    <a href="blg-fabric-ml-success-strategies-from-start-2-finish-part-3.html">Part-3</a>
                </div>



            </div>
        </div>
    </div>
</div>


        <!-- Services End -->


        <!-- Footer Start -->
        <div id="footer"></div>
        <script src="dislmr/footer-bar.js"></script>
        <script>
        loadFooter(); // or loadFooter('footer') if you want to be explicit
        </script>
        <!-- Footer End -->


        <!-- Back to Top -->
        <a href="#" class="btn btn-secondary btn-square rounded-circle back-to-top"><i class="fa fa-arrow-up text-white"></i></a>

        
        <!-- JavaScript Libraries -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"></script>
        <script src="lib/wow/wow.min.js"></script>
        <script src="lib/easing/easing.min.js"></script>
        <script src="lib/waypoints/waypoints.min.js"></script>
        <script src="lib/owlcarousel/owl.carousel.min.js"></script>

        <!-- Template Javascript -->
        <script src="js/main.js"></script>
    </body>


</html>




